1.What is the difference between Analysis and Analytics
Ans.In simple terms, Data Analytics is the process of exploring the data from the past to make appropriate decisions in the future by using valuable insights.
    Whereas Data Analysis helps in understanding the data and provides required insights from the past to understand what happened so far.

2.Define Data Science in your own words
Ans.Data Science is a blend of various fields like Probability, Statistics, Programming, Analysis, Cloud Computing, etc; which are used to extract valuable insights from 
    the data provided. It is a vast field which is a  booming field and all people are learning these skills to become a professional in this domain.

3.What is the meaning of Machine Learning
Ans.Artificial Intelligence is purely math and scientific exercise but when it becomes computational, it starts to solve human problems.
    Machine Learning is a subset of Artificial Intelligence. ML is the study of computer algorithms that improve automatically through experience. 
    ML explores the study and construction of algorithms that can learn from data and make predictions on data. Based on more data, machine learning can change 
    actions and responses which will make it more efficient, adaptable, and scalable.

4.What is generalisation and IID ? How is it used in predictions ?
Ans.The generalization of machine learning models is the ability of a model to classify or forecast new data. When we train a model on a dataset, and the model is 
    provided with new data absent from the trained set, it may perform well. Such a model is generalizable
    Benefits:Sometimes generalization can be a process to improve performance. In deep learning, models can analyze and understand patterns present in datasets. 
             They are also liable to overfitting. Using generalization techniques, this overfitting can be managed so that the model is not too strict. 
             It can assist in deep learning to predict a pattern that has not been seen before

IID?


5.What is a model ?
Ans.A machine learning model is a file that has been trained to recognize certain types of patterns. 
    Models use machine learning algorithms, during which the machine learns from the data just like humans learn from their experiences. 
    Machine learning models can be broadly divided into two categories based on the learning algorithm which can further be classified based on the task 
    performed and the nature of the output.

6.Explain, in brief, a model building process (in general)
Ans.#)Read Business problem-->#)Import packages-->#)Import Data-->#)Data undersatnading-->#)EDA-->Univariate/Biavariate/Mutivariate-->#Data Cleaning-->#Feature Engg.-->
    #)Label Encoding-->#)Scaling if required-->#)Key performance Indicator-->#)Dummy-->#)Training Model-->#)Training Dataset-->#)Testing Dataset-->#)Testing Model-->
    #)Deploy Model

7.What are parametric and non-parametric models ?
Ans.A learning model that summarizes data with a set of fixed-size parameters (independent on the number of instances of training).Parametric machine learning algorithms 
    are which optimizes the function to a known form.In a parametric model, you know exactly which model you are going to fit in with the data, for example, linear 
    regression line.b0 + b1*x1 + b2*x2 = 0 where,
    b0, b1, b2 → the coefficients of the line that control the intercept and slope x1, x2 → input variables ex-Logistic Regression,Linear Discriminant Analysis,Perceptron,
    Naive Bayes
    
    Nonparametric machine learning algorithms are those which do not make specific assumptions about the type of the mapping function. They are prepared to choose any
    functional form from the training data, by not making assumptions.The parameters are adjustable and can change.When dealing with ranked data one may turn to nonparametric
    modeling, in which the sequence in that they are ordered is some of the significance of the parameters.Ex-k-Nearest Neighbors,Decision Trees like CART and C4.5,
    Support Vector Machines

   Parametric models deal with discrete values, and nonparametric models use continuous values.
   A parametric model can predict future values using only the parameters. While nonparametric machine learning algorithms are often slower and require large amounts 
   of data, they are 
   rather flexible as they minimize the assumptions they make about the data.

8.Difference between Supervised, Unsupervised and Time-Series models
Ans.The main difference between supervised and unsupervised learning: Labeled data
    The main distinction between the two approaches is the use of labeled datasets. To put it simply, supervised learning uses labeled input and output data, while an 
    unsupervised learning algorithm does not.
  
    Time series is a statistical modelnot typical ML model.Order of data is imp.Modelling relationships of data collected over a period of time.

9.Explain the process of Linear Regression
Ans. Linear regression shows the linear relationship between the independent(predictor) variable i.e. X-axis and the dependent(output) variable i.e. Y-axis(Continuous data), 
     called linear regression.
     If there is a single input variable X(independent variable), such linear regression is called simple linear regression.
     the best fit line is obtained by minimizing the Residual Sum of Squares(RSS).sum(y-yhat(pred))^2
     Coefficient of Determination or R-Squared (R2)=1-RSS/TSS,TSS=sum(y-ybar(mean))^2,Root Mean Squared Error (RSME) and Residual Standard Error (RSE)

10.What are the assumptions of Linear regression ?
Ans. 1.Linearity of residuals: There needs to be a linear relationship between the dependent variable and independent variable(s).
     2.Independence of residuals: The error terms should not be dependent on one another (like in time-series data wherein the next value is dependent on the previous one).
       There should be no correlation between the residual terms. The absence of this phenomenon is known as Autocorrelation.
     3.Normal distribution of residuals
     4. The equal variance of residuals: The error terms must have constant variance. This phenomenon is known as Homoscedasticity.

11.How will you check and handle multicollinearity ?
Ans.Multicollinearity occurs when two or more independent variables are highly correlated with one another in a regression model.This means that an independent variable 
    can be predicted from another independent variable in a regression model. For example, height and weight, household income and water consumption
    
    Dropping one of the correlated features will help in bringing down the multicollinearity between correlated features,determine by:VIF (Variable Inflation Factors)

12.How will you check and handle heteroscedasticity ?
Ans.The presence of non-constant variance in the error terms is referred to as Heteroscedasticity. 
    transform the response variable such as log(Y) or √Y. Also, you can use weighted least square method to tackle heteroskedasticity.

13.How will you evaluate a Linear regression model ?
Ans.Evaluation metrics such as R2Score(Coef. of determination)[Higher the value better the model],RMSE(lower the value better the model)

14.What are the different variations of a Linear Regression model ?
Ans.Simple Linear Regression,
    Multiple  Linear Regression-Multiple linear regression is a technique to understand the relationship between a single dependent variable and 
    multiple independent variables.
    Y = B0 + B1X1 + B2X2 + … + BpXp + ε
    All the four assumptions made for Simple Linear Regression still hold true for Multiple Linear Regression along with a few new additional assumptions.
    Overfitting: When more and more variables are added to a model, the model may become far too complex and usually ends up memorizing all the data points in 
    the training set. This phenomenon is known as the overfitting of a model. This usually leads to high training accuracy and very low test accuracy.
    Multicollinearity: It is the phenomenon where a model with several independent variables, may have some variables interrelated.
    Feature Selection: With more variables present, selecting the optimal set of predictors from the pool of given features (many of which might be redundant) 
    becomes an important task for building a relevant and better model.

15.Explain the process of Logistic Regression
Ans.Logistic Regression is a “Supervised machine learning” algorithm that can be used to model the probability of a certain class or event. It is used when the 
    data is linearly separable and the outcome is binary or dichotomous in nature.
    Binary Classification refers to predicting the output variable that is discrete in two classes.

16.Why can't we use the formula for Linear Regression for a Binary classification (Logistic Regression) ?
Ans.the predicted value may exceed the range (0,1),error rate increases if the data has outliers
link-https://www.analyticsvidhya.com/blog/2021/07/an-introduction-to-logistic-regression/#:~:text=Logistic%20Regression%20is%20a%20%E2%80%9CSupervised,binary%20or%20dichotomous%20in%20nature.
    p = b0+b1x     --------> eq 1
    The right-hand side of the equation (b0+b1x) is a linear equation and can hold values that exceed the range (0,1). But we know probability will always be in the 
    range of (0,1).To overcome that, we predict odds instead of probability.
    Odds: The ratio of the probability of an event occurring to the probability of an event not occurring.
    Odds = p/(1-p)
    The equation 1 can be re-written as: p/(1-p) = b0+b1x      --------> eq 2
    Odds can only be a positive value, to tackle the negative numbers, we predict the logarithm of odds.
    Log of odds = ln(p/(1-p))
    The equation 2 can be re-written as:ln(p/(1-p)) = b0+b1x      --------> eq 3
    p/(1-p) = e(b0+b1x)-->after modification p = 1/ (1 + e-(b0+b1x1+b2x2+b3x3+----+bnxn)
    it is the sigmoid function. It helps to squeeze the output to be in the range between 0 and 1.
     p = 1/ (1 + e^-z)


17.How do you interpret dummy variable features from the summary of Logistic Regression ?
Ans.

18.How is it different from numerical features interpretation ?
Ans.

19.What are the assumptions of Logistic regression ?
Ans.1. It assumes that there is minimal or no multicollinearity among the independent variables i.e, predictors are not correlated.

   2. There should be a linear relationship between the logit of the outcome and each predictor variable. The logit function is described 
     as logit(p) = log(p/(1-p)), where p is the probability of the target outcome.

   3. Sometimes to predict properly, it usually requires a large sample size.

   4. The Logistic Regression which has binary classification i.e, two classes assume that the target variable is binary, and ordered Logistic 
      Regression requires the target variable to be ordered.

20.How do you evaluate a binary classification model ?
Ans.Accuracy-Accuracy is useful when the target class is well balanced 
    
    Precision-Precision for a label is defined as the number of true positives divided by the number of predicted positives.
    
    Recall-Recall for a label is defined as the number of true positives divided by the total number of actual positives.  
     
    F1 Score is the weighted harmonic mean of precision and recall
    F1-Score=2PxR/P+R
    
    AUC-ROC — The Receiver Operator Characteristic (ROC) is a probability curve that plots the TPR(True Positive Rate) against the FPR(False Positive Rate) at 
    various threshold values and separates the ‘signal’ from the ‘noise’.
      
    Confusion Matrix is a performance measurement for the machine learning classification problems where the output can be two or more classes. 
    It is a table with combinations of predicted and actual values.
    It is extremely useful for measuring the Recall, Precision, Accuracy, and AUC-ROC curves.

    True Positive: We predicted positive and it’s true. In the image, we predicted that a woman is pregnant and she actually is.

    True Negative: We predicted negative and it’s true. In the image, we predicted that a man is not pregnant and he actually is not.

    False Positive (Type 1 Error)- We predicted positive and it’s false. In the image, we predicted that a man is pregnant but he actually is not.

    False Negative (Type 2 Error)- We predicted negative and it’s false. In the image, we predicted that a woman is not pregnant but she actually is.
 

21.Why is Confusion Matrix named like this ?
Ans.

22.From a confusion matrix output, how can you determine the actual and predicted values ?
Ans.-----------------
        |  Actual   |
  -------------------
        |    1   0  |
  pred  |1 | TP  FP |
  values|0 | FN  TN |
  -------------------
23.Why Accuracy alone is not a good measure to determine a classification model goodness ?
Ans.Accuracy is usefull when the target class is well balanced or normally distributed.And F1-Score is good for imbalanced classes.


24.What are imbalanced classes ? 
Ans.Class imbalanced is generally normal in classification problems. But, in some cases, this imbalance is quite acute where the majority class’s 
    presence is much higher than the minority class.

25.How will you handle them in a dataset ?
Ans.1.F1-Score is good for imbalanced classes;F1-Score=2PxR/P+R
    So, if the classifier predicts the minority class but the prediction is erroneous and false-positive increases, the precision metric will be 
    low and so as F1 score. Also, if the classifier identifies the minority class poorly, i.e. more of this class wrongfully predicted as the majority 
    class then false negatives will increase, so recall and F1 score will low. F1 score only increases if both the number and quality of prediction improves.

    F1 score keeps the balance between precision and recall and improves the score only if the classifier identifies more of a certain class correctly.
     
    2.Resampling (Oversampling and Undersampling)-->Sklearn.utils import resample
      When we are using an imbalanced dataset, we can oversample the minority class using replacement. This technique is called oversampling.
      Similarly, we can randomly delete rows from the majority class to match them with the minority class which is called undersampling.
      
      After sampling the data we can get a balanced dataset for both majority and minority classes. So, when both classes have a similar number of records present 
      in the dataset, we can assume that the classifier will give equal importance to both classes.

    3.SMOTE:-->imblearn.over_sampling import SMOTE
      Synthetic Minority Oversampling Technique or SMOTE is another technique to oversample the minority class. Simply adding duplicate records of minority class 
      often don’t add any new information to the model. In SMOTE new instances are synthesized from the existing data. If we explain it in simple words, SMOTE looks  
      into minority class instances and use k nearest neighbor to select a random nearest neighbor, and a synthetic instance is created randomly in feature space.
      
26.Implications of imbalance in predictions ?
Ans.Let’s explain it with an example of disease diagnosis. Let’s assume we are going to predict disease from an existing dataset where for every 100 records only 5 
    patients are diagnosed with the disease. So, the majority class is 95% with no disease and the minority class is only 5% with the disease. Now, assume our model 
    predicts that all 100 out of 100 patients have no disease.

27.Difference between LabelEncoder and One-hot encoding. Why is it important ?
Ans.A machine can only understand the numbers. It cannot understand the text.That’s primarily the reason we need to convert categorical columns to numerical columns so 
    that a machine learning algorithm understands it. This process is called categorical encoding. 
     
    label encoding uses alphabetical ordering. Hence, India has been encoded with 0, the US with 2, and Japan with 1.
    In the above scenario, the Country names do not have an order or rank. But, when label encoding is performed, the country names are ranked based on the alphabets.
    Due to this, there is a very high probability that the model captures the relationship between countries such as India < Japan < the US.
    Here comes the concept of One-Hot Encoding.

    One-Hot Encoding is another popular technique for treating categorical variables. It simply creates additional features based on the number of unique values in the 
    categorical feature. Every unique value in the category will be added as a feature.

    One-Hot Encoding is the process of creating dummy variables
    Challenges with one hot-->The Dummy Variable Trap leads to the problem known as multicollinearity. Multicollinearity occurs where there is a dependency between the independent features. 
    Multicollinearity is a serious issue in machine learning models like Linear Regression and Logistic Regression.Use Variance Inflation Factor(VIF) and drop column with high VIF

28.How can you determine which algorithm to use for a given problem ?
Ans.We apply One-Hot Encoding when:

    The categorical feature is not ordinal (like the countries above)
    The number of categorical features is less so one-hot encoding can be effectively applied
    
    We apply Label Encoding when:

    The categorical feature is ordinal (like Jr. kg, Sr. kg, Primary school, high school)
    The number of categories is quite large as one-hot encoding can lead to high memory consumption

29.Where are distance formula used ?
Ans.Several machine learning algorithms like k-nearest neighbors for supervised learning and k-means clustering for unsupervised learning uses Distance metrics 

30.Generally, data is scaled for algorithms using distance formula. Why ?
Ans.If value of column is in lacs ans another in tens or hundred than the distances to calculate similarity b/w two points we be driven by column having lacs value.
    Algorithm will biased toward higher magnitude so before applying alogrithm we scale data to remove biasedness.
    Two method to scaling the data is
    1.Standard Scaler-->Normalization=values b/w [-1,1]
    2.Min-Max Scaler= values b/w [0,1]


30.On what basis is the splitting ratio (to split data into train/test/validation) decided ?
Ans.It totally depend on size of data depending on it we go for splitting.Generally train set is the largest as to cover all type of variation in data.LArge the train set more
    effective will be our model.train=70%,validate=15%/10%,test=15%/20%

31.How can a model be tuned for better performce ?
Ans. 1.Add more data 2.treat missing values/ouliers 3.feature engg. 4.Feature Selection 5.Multiple Alogrithm 6.Alogrithm Tuning 7.Ensemble methods-->Bagging/Bossting
    Using Hyper parameter tuning,cross-validation,grid-search CV


32.What is hyper-parameter tuning ? 
Ans.Hyperparameter tuning is basically referred to as tweaking the parameters of the model, which is basically a prolonged process.
     the performance of the machine learning model improves with a more acceptable choice of hyperparameter tuning

33.Give a couple of examples
Ans.Logistic Regre-->C=1/lambda;lambda=regularization parameter,L1=Lasso,L2=Ridge (penalties)
    KNN-->n_neighbors,p=1-->manhattan dist;p=2-->euclidean distance
    PCA,LDA etc.

34.What is the use of GridSearch ?
Ans. GridSearch Library itself perform the search operations and returns the performing model and its score. In which each model are built for each permutation of a given hyperparameter, 
    internally it would be evaluated and ranked across the given cross-validation folds.

35.What is feature selection ? 
Ans.feature selection techniques in machine learning is to find the best set of features that allows one to build useful models of studied phenomena.


36.How can you select the best features for a dataset ?
Ans.Filter Methods-->1.Correlation Matrix, 2.Chi-square test 3.Information Gain


37.What is forward and back propogation in model building ?
Ans.Wrapper method of feature selection
    Forward feature Seleection-This is an iterative method wherein we start with the best performing variable against the target. Next, we select another variable 
    that gives the best performance in combination with the first selected variable. This process continues until the preset criterion is achieved

    Backward Selection-This method works exactly opposite to the Forward Feature Selection method. Here, we start with all the features available and build a model.
    Next, we the variable from the model which gives the best evaluation measure value. This process is continued until the preset criterion is achieved.


38.Difference between k-means and k-NN ?
Ans.KNN i.e K-Nearest Neighbors is a supervised classification algorithm used to classify datapoints into different categories say category-alpha and category-beta

    K-Means is an unsupervised algorithm used to cluster together similar datapoints. Unlike supervised learning mechanisms, we use unlabeled data. The algorithm relies 
    on the independent features of the underlying data. K-Means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster 
    analysis in data mining. K-Means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean


39.If using k-NN for a binary classification model, we generally prefer an odd number for k. Why ?
Ans.If you are using K and you have an even number of classes (e.g. 2) it is a good idea to choose a K value with an odd number to avoid a tie. And the inverse,  
    use an even number for K when you have an odd number of classes.

40.What is the difference between Bagging and Boosting ?
Ans.Bagging is a homogeneous weak learners’ model that learns from each other independently in parallel and combines them for determining the model average. 

     

    Bagging models are better to avoid overfitting but will rarely get a better bias. On the other hand, boosting can generate a model with low errors but is prone to 
    being more overfit

41.For Linear Regression problems, Decision Tree regressor is generally not preferred. Why ?
Ans.

42.What is the use of Cross Validation ?
Ans.Cross-Validation is a resampling technique with the fundamental idea of splitting the dataset into 2 parts- training data and test data. Train data is used to train
    the model and the unseen test data is used for prediction. If the model performs well over the test data and gives good accuracy, it means the model hasn’t overfitted 
    the training data and can be used for prediction.

Explain each Cross Validation technique in detail?
Ans.1.Hold out Method:
    Here the entire dataset(population) is divided into 2 sets – train set and test set. The data can be divided into 70-30 or 60-40, 75-25 or 80-20, or even 50-50 depending 
    on the use case. As a rule, the proportion of training data has to be larger than the test data.
    Drawback:a)In the Hold out method, the test error rates are highly variable (high variance) and it totally depends on which observations end up in the training 
               set and test set
             b)Only a part of the data is used to train the model (high bias) which is not a very good idea when data is not huge and this will lead to overestimation 
               of test error.

    2.Leave one out method (LOOM
     In this method, we divide the data into train and test sets – but with a twist. Instead of dividing the data into 2 subsets, we select a single observation as 
     test data, and everything else is labeled as training data and the model is trained. Now the 2nd observation is selected as test data and the model is trained on the remaining data.
     Draback:This process continues ‘n’ times and the average of all these iterations is calculated and estimated as the test set error.When it comes to test-error estimates, LOOCV gives
     unbiased estimates (low bias). But bias is not the only matter of concern in estimation problems. We should also consider variance.
     LOOCV has an extremely high variance because we are averaging the output of n-models which are fitted on an almost identical set of observations, and their outputs are highly positively
      correlated with each other.
     And you can clearly see this is computationally expensive as the model is run ‘n’ times to test every observation in the data. Our next method will tackle this problem and give us a good 
     balance between bias and variance.


43.What is k-fold CV ?
Ans.In this resampling technique, the whole data is divided into k sets of almost equal sizes. The first set is selected as the test set and the model is trained on the remaining k-1 sets. 
    The test error rate is then calculated after fitting the model to the test data.
    In the second iteration, the 2nd set is selected as a test set and the remaining k-1 sets are used to train the data and the error is calculated. This process continues for all the k sets.

    The mean of errors from all the iterations is calculated as the CV test error estimate.

    In K-Fold CV, the no of folds k is less than the number of observations in the data (k<n) and we are averaging the outputs of k fitted models that are somewhat less correlated with each other 
    since the overlap between the training sets in each model is smaller. This leads to low variance then LOOCV.
    
   The major disadvantage of this method is that the model has to be run from scratch k-times and is computationally expensive than the Hold Out method but better than the Leave One Out method. 
 
    Stratified K-fold
    This is a slight variation from K-Fold Cross Validation, which uses ‘stratified sampling’ instead of ‘random sampling.’
    Taking example which has a cosmetic product review of 1000 customers out of which 60% is female and 40% is male. I want to split the data into train and test data in proportion (80:20). 80% of 1000 
    customers will be 800 which will be chosen in such a way that there are 480 reviews associated with the female population and 320 representing the male population. In a similar fashion, 20% of 1000 
    customers will be chosen for the test data 
    
    This is exactly what stratified K-Fold CV does and it will create K-Folds by preserving the percentage of sample for each class. This solves the problem of random sampling associated with Hold out 
    and K-Fold methods.


44.How is clustering different from classification ?
Ans.Clustering is alogrithm of unsupervised ML where as classification is alogrithm used in supervised learning.

45.Evaluate a clustering algorithm
Ans.It can be done by analysing scatter plot of cluster and by high value of silhoutte score for specific number of cluster(k)

46.How do you split data into train and test for Time Series ?
Ans.By row indexing we can split the data into train-test split, here random split can't be done as it is a series data.

47.What are the different ways to take data samples ?
Ans.
48.Why is Naïve Bayes called "Naïve" ?
Ans.Naive is there as we are assuming all features/columns to be independent of each other which is not true.

50.What are the challenges involved while processing unstructured data ?
Ans.There are multiple challenges faced while working with unstructured data, namely:

   This type of data is raw and unorganized.
   It is difficult to find out if the data is relevant.
   Finding high-quality data is tricky.
   Searching for info and indexing is a challenge.
   More processing is required.

51.What are bag-of-words ?
Ans.The Bag of Words (BoW) model is the simplest form of text representation in numbers. Like the term itself, we can represent a sentence as a bag of words vector

52.What is the main use of stop words ?
Ans.Stopwords are the most common words in any natural language. For the purpose of analyzing text data and building NLP models, these stopwords might not add 
    much value to the meaning of the document.

53.Describe, in brief, how sentiment analysis is done on a text
Ans.Sentiment Analysis is a use case of Natural Language Processing (NLP) and comes under the category of text classification.
    Text Blob is a Python library for Natural Language Processing. Using Text Blob for sentiment analysis is quite simple. It takes text as an input and can return polarity and subjectivity as outputs.
    
    Polarity determines the sentiment of the text. Its values lie in [-1,1] where -1 denotes a highly negative sentiment and 1 denotes a highly positive sentiment.
    
    Subjectivity determines whether a text input is factual information or a personal opinion. Its value lies between [0,1] where a value closer to 0 denotes a piece 
    of factual information and a value closer to 1 denotes a personal opinion.

54.Is TF-IDF a good measure to measure word importance ?
Ans.Term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.
    
    Yes it is good measure as it capture context mathematically. And higher the value more will be text relevancy


55.Alternate method
Ans.One hot econding
    BOG
    N-Gram

56.What is Cosine similarity measure used for ?
Ans.Cosine Similarity is measured for semantic order to find vector computation vector

57.What is part-of-speech tagging ?
Ans.Part-of-Speech(POS) Tagging is the process of assigning different labels known as POS tags to the words in a sentence that tells us about the part-of-speech of the word.
    Like the word is adject.,noun,adverb,verb,pronoun etc

58.What are identifiers and modifiers w.r.t. regular expressions ?
Ans.

59.Challenges when extracting data from web pages ?

60.What are Odds ? Where is it used in ML ?
Ans.Odds are basically the ratio of some event happening to some event not happening. It can also be defined as the ratio of the probability of an event happening to the Probability of the event not happening. 
    Odds can be expressed as a Ratio or a Fraction.

    It is used Logistic Regression

61.Difference between covariance and correlation
Ans.Covariance is a metric that gives us the relationship between two random variables. This metric evaluates how much the variables change together. 
    It is nothing but a measure of the variance between two variables.
    
   Correlation is a metric that can show whether and how strongly a pair of random variables are related.Correlation is an extended metric of covariance.

62.How does Decision tree split on a node ?
Ans.

63.Which is more severe: Type I or Type II error ?
Ans.Type I Error is more sevre as it False Positive Prediction which is dangerous

64.What is feature engineering ?
Ans.Feature engineering is a very important aspect of machine learning and data science and should never be ignored. The main goal of Feature engineering is to get
    the best results from the algorithms.
    Feature Selection
    Handling missing values
    Handling imbalanced data
    Handling outliers
    Binning
    Encoding
    Feature Scaling

65.What is R2  and Adjusted R2 ? Which is a stable measure ?
Ans.First it will test all the independent feature and than it will choose the best feature for the model building purpose.But R2 doesn't consider this it will take all    
    features either they are contributing or not.

66.What is overfitting and underfitting ?
Ans.This situation where any given model is performing too well on the training data but the performance drops significantly over the test set is called an overfitting model.

    On the other hand, if the model is performing poorly over the test and the train set, then we call that an underfitting model.

67.People who bought this, also bought this' 
Ans.This is an example of Recommendation system

Difference between Tall and Wide format data
68.How will you interpret a Linear Regression formula ?
Ans.Y= B0 + B1*X.
   This algorithm explains the linear relationship between the dependent(output) variable y and the independent(predictor) variable X using a straight line.

69.How will you interpret a Logistic Regression formula ?
Ans. Linear model: ŷ = b0+b1x
     Sigmoid function: σ(z) = 1/(1+e^−z)
     Logistic regression model: ŷ = σ(b0+b1x) = 1/(1+e^-(b0+b1x))

70.What is a stationary data in Time Series ?
Ans. A dataset which is having constant variance and mean over a period of time.To forecast there should be no trend in data.

71.How do you make a data stationary, in case the data shows trends ?
Ans.By double smoothning we can remove trend from our dataset

72.What problem does Random Forest in comparison to a Decision Tree ?
Random Forest addresses several problems compared to a single Decision Tree:

• Overfitting: Decision Trees are prone to overfitting, while Random Forest reduces overfitting by building an ensemble of trees trained on random subsets of the data.

• Variance Reduction: Decision Trees can have high variance, but Random Forest averages predictions from multiple trees to reduce variance.

• Interpretability: Decision Trees are more interpretable than Random Forests, as they represent a single set of rules, whereas Random Forests consist of multiple trees.

• Computational Complexity: Random Forest is more computationally intensive than a single Decision Tree due to the ensemble approach, which can be a consideration for large datasets or   
  resource-constrained environments.

73.What are the 2 main properties of a cluster ?
Ans.Two main properties of clusters are:
• Centrality: This property refers to the notion that each cluster has a central point or centroid that represents the "center" of the cluster. This centroid can be calculated based on 
  the mean, median, or other measures depending on the clustering algorithm used.

• Similarity: Clusters are formed based on the similarity or proximity of data points within the cluster. Data points within the same cluster should be more similar to each other than to  
  data points in other clusters. The measure of similarity can vary depending on the specific clustering algorithm and the type of data being clustered.

74.How do you determine optimum clusters ?
Ans.We can determine optimum cluster in three ways:
    Elbow curve(K-Means)
    k-distance graph(DBSCAN)
    Dendogram (Agglomerative Clustering)
    Silhoutte score

75.What is a dendrogram ?
Ans.A dendrogram is a tree-like diagram that records the sequences of merges or splits.

76.What is a correlogram ?
Ans.A plot of autocorrelation coefficients on the vertical axis with different lags on the horizontal axis is termed a correlogram.
    Periodicities in a time series can be seen easily in a correlogram as time values at which the autocorrelation coefficient reapproaches 1.00

76.What problems are likely to arise when there is a severe imbalance in multiclass classification ?
Ans.


Question: F1-score is a widely used evaluation metric for classification models and is calculated by taking a harmonic mean of Precision and Recall. One very fundamental question asked in many Data Science interviews is that "𝗪𝗵𝘆 𝗱𝗼 𝘄𝗲 𝘂𝘀𝗲 𝗛𝗮𝗿𝗺𝗼𝗻𝗶𝗰 𝗺𝗲𝗮𝗻, 𝘄𝗵𝘆 𝗰𝗮𝗻'𝘁 𝘄𝗲 𝘂𝘀𝗲 𝘀𝗶𝗺𝗽𝗹𝗲 𝗮𝘃𝗲𝗿𝗮𝗴𝗲 𝗼𝗳 𝗽𝗿𝗲𝗰𝗶𝘀𝗶𝗼𝗻 𝗮𝗻𝗱 𝗥𝗲𝗰𝗮𝗹𝗹?"

👉 F1-score represents a balanced accuracy metric which summarizes both precision and recall into one single value, ranging from 0 to 1. F1-score is defined as the Harmonic mean of Precision and Recall. 𝗧𝗵𝗶𝘀 𝗶𝘀 𝗱𝗼𝗻𝗲 𝘁𝗼 𝗲𝗻𝘀𝘂𝗿𝗲 𝘁𝗵𝗮𝘁 𝘁𝗵𝗲 𝘃𝗮𝗹𝘂𝗲 𝗶𝘀 𝗺𝗲𝗮𝗻𝗶𝗻𝗴𝗳𝘂𝗹 𝗮𝗻𝗱 𝗻𝗼𝘁 𝗯𝗶𝗮𝘀𝗲𝗱 𝘁𝗼𝘄𝗮𝗿𝗱𝘀 𝗲𝘅𝘁𝗿𝗲𝗺𝗲𝗹𝘆 𝗹𝗮𝗿𝗴𝗲 𝘃𝗮𝗹𝘂𝗲𝘀. One way to achieve this is to make the value (F1-score) conservative by bringing it closer to the smaller of the two and Harmonic Mean does exactly that.

👉 But why does it make sense? Consider a scenario of predicting whether a patient is diagnosed with Cancer or not (binary classification setting). Let’s say, the test dataset is highly skewed with 2 positive cases (cancer patients) and 98 negative cases.

👉Assume that there is a model which always predicts all cases in test dataset to be negative (non-cancer patients). In such a scenario, precision would be 0.98 (98 correct predictions out of 100) and recall = 0 (as it did not predict a single patient as cancer patient). If we take an average of precision and recal here, it would be 0.49 and if we take their harmonic mean, it would be 0. If you think carefully, the model under consideration is of no good as it is just predicting blindly everyone as non-cancer patient, still achieving a score of 0.49 (in the former case). However, the latter case (with harmonic mean) more meaningfully represents the model quality. So, to avoid such situation, Harmonic mean is always a better choice.


Question: "𝗪𝗵𝗮𝘁 𝗶𝘀 𝘁𝗵𝗲 𝗱𝗶𝗳𝗳𝗲𝗿𝗲𝗻𝗰𝗲 𝗯𝗲𝘁𝘄𝗲𝗲𝗻 𝗟𝗼𝘀𝘀 𝗳𝘂𝗻𝗰𝘁𝗶𝗼𝗻 𝗮𝗻𝗱 𝗖𝗼𝘀𝘁 𝗳𝘂𝗻𝗰𝘁𝗶𝗼𝗻 𝗶𝗻 𝘁𝗵𝗲 𝗰𝗼𝗻𝘁𝗲𝘅𝘁 𝗼𝗳 𝗠𝗮𝗰𝗵𝗶𝗻𝗲 𝗟𝗲𝗮𝗿𝗻𝗶𝗻𝗴?"

👉 While both Loss function and Cost function are used to calculate error in the predictions of an ML algorithm, there is a subtle difference as to how they are used.

👉 𝗟𝗼𝘀𝘀 𝗳𝘂𝗻𝗰𝘁𝗶𝗼𝗻 is generally used when we are interested in measuring the error between the prediction and the actual with respect to a 𝘀𝗶𝗻𝗴𝗹𝗲 𝗼𝗯𝘀𝗲𝗿𝘃𝗮𝘁𝗶𝗼𝗻. On the other hand, when the error is measured across the entire data set under consideration, we use the term 𝗖𝗼𝘀𝘁 𝗳𝘂𝗻𝗰𝘁𝗶𝗼𝗻.

👉 In other words, 𝗖𝗼𝘀𝘁 𝗳𝘂𝗻𝗰𝘁𝗶𝗼𝗻 𝗮𝗴𝗴𝗿𝗲𝗴𝗮𝘁𝗲𝘀 𝘁𝗵𝗲 𝗹𝗼𝘀𝘀 𝘃𝗮𝗹𝘂𝗲𝘀 𝘁𝗵𝗮𝘁 𝗵𝗮𝘃𝗲 𝗮𝗹𝗿𝗲𝗮𝗱𝘆 𝗯𝗲𝗲𝗻 𝗰𝗮𝗹𝗰𝘂𝗹𝗮𝘁𝗲𝗱 𝗽𝗲𝗿 𝗼𝗯𝘀𝗲𝗿𝘃𝗮𝘁𝗶𝗼𝗻.